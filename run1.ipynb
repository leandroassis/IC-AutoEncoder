{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os import getcwd\n",
    "\n",
    "path.insert(getcwd() + '/modules/') # adding modules folder to path\n",
    "\n",
    "from modules.DataMod import DataSet\n",
    "from modules.TrainingManager import KerasTrainingManager\n",
    "from modules.TrainingFunctions import generator_training\n",
    "from modules.misc import LSSIM, ssim_metric, psnrb_metric\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the datasets\n",
    "tinyDataSet = DataSet.load_rafael_tinyImagenet_64x64_noise_data()\n",
    "cifarDataSet = DataSet.load_rafael_cifar_10_noise_data()\n",
    "cifarAndTinyDataSet = DataSet.concatenateDataSets(cifarDataSet, tinyDataSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: paralelize the training (does it's necessary?)\n",
    "\n",
    "# uNet\n",
    "\n",
    "# trainiing unet only with tinyImagenet dataset\n",
    "uNet_tiny_Manager = KerasTrainingManager(\n",
    "    \"Unet2.3-64x64.json\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'batch_size' : 20, 'epochs' : 15, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM],\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = tinyDataSet\n",
    ")\n",
    "\n",
    "# trainiing unet only with cifar dataset\n",
    "uNet_cifar_Manager = KerasTrainingManager(\n",
    "    \"Unet2.3-64x64.json\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'batch_size' : 20, 'epochs' : 15, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM], # not sure about it\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = cifarDataSet\n",
    ")\n",
    "\n",
    "# training unet with both datasets concatenated\n",
    "uNet_cifarAndTiny_Manager = KerasTrainingManager(\n",
    "    \"Unet2.3-64x64.json\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'batch_size' : 20, 'epochs' : 15, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM],\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = cifarAndTinyDataSet\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# autoencoder\n",
    "\n",
    "# training autoencoder only with tinyImagenet dataset\n",
    "autoencoder_tiny_Manager = KerasTrainingManager(\n",
    "    \"AutoEncoder-2.3-64x64\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'batch_size' : 20, 'epochs' : 15, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM],\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = tinyDataSet\n",
    ")\n",
    "\n",
    "# training autoencoder only with cifar dataset\n",
    "autoencoder_cifar_Manager = KerasTrainingManager(\n",
    "    \"AutoEncoder-2.3-64x64\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'batch_size' : 20, 'epochs' : 15, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM],\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = cifarDataSet\n",
    ")\n",
    "\n",
    "# training autoencoder with both datasets concatenated\n",
    "autoencoder_cifarAndTiny_Manager = KerasTrainingManager(\n",
    "    \"AutoEncoder-2.3-64x64\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'batch_size' : 20, 'epochs' : 15, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM],\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = cifarAndTinyDataSet\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual autoencoder\n",
    "residual_autoencoder_tiny_Manager = KerasTrainingManager(\n",
    "    \"ResidualAutoEncoder-0.1-64x64\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'max_val': 255, 'filter_size': 9, 'filter_sigma': 1.5, 'k1':0.01, 'k2': 0.03, 'batch_size' : 20, 'epochs' : 25, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM], # not sure about it\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = tinyDataSet\n",
    ")\n",
    "residual_autoencoder_cifar_Manager = KerasTrainingManager(\n",
    "    \"ResidualAutoEncoder-0.1-64x64\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'max_val': 255, 'filter_size': 9, 'filter_sigma': 1.5, 'k1':0.01, 'k2': 0.03, 'batch_size' : 20, 'epochs' : 25, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM], # not sure about it\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = cifarDataSet\n",
    ")\n",
    "\n",
    "residual_autoencoder_cifarAndTiny_Manager = KerasTrainingManager(\n",
    "    \"ResidualAutoEncoder-0.1-64x64\",\n",
    "    optimizer = Adam,\n",
    "    optimizer_kwargs = {'learning_rate' : 0.001, 'beta_1' : 0.9, 'beta_2' : 0.999, 'epsilon' : 1e-7, 'amsgrad' : False},\n",
    "    \n",
    "    fit_kwargs = {'max_val': 255, 'filter_size': 9, 'filter_sigma': 1.5, 'k1':0.01, 'k2': 0.03, 'batch_size' : 20, 'epochs' : 25, 'verbose':1, 'validation_split':0, 'shuffle':True, \n",
    "    'class_weight':None, 'sample_weight':None, 'steps_per_epoch':None, 'validation_steps':None, \n",
    "    'validation_batch_size':None, 'validation_freq':1, 'max_queue_size':10, 'workers':1, 'use_multiprocessing':False},\n",
    "\n",
    "    metrics = [ssim_metric, psnrb_metric, LSSIM], # not sure about it\n",
    "\n",
    "    training_function = generator_training,\n",
    "\n",
    "    dataset = cifarAndTinyDataSet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting models training \n",
    "uNet_tiny_Manager.start_training()\n",
    "uNet_cifar_Manager.start_training()\n",
    "uNet_cifarAndTiny_Manager.start_training()\n",
    "\n",
    "autoencoder_tiny_Manager.start_training()\n",
    "autoencoder_cifar_Manager.start_training()\n",
    "autoencoder_cifarAndTiny_Manager.start_training()\n",
    "\n",
    "residual_autoencoder_tiny_Manager.start_training()\n",
    "residual_autoencoder_cifar_Manager.start_training()\n",
    "residual_autoencoder_cifarAndTiny_Manager.start_training()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
