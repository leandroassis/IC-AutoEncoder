{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os import getcwd, environ\n",
    "\n",
    "path.insert(0, getcwd())\n",
    "path.insert(0, getcwd() + \"/modules\")\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\"\n",
    "\n",
    "from modules.DataMod import DataSet\n",
    "from modules.CustomLosses import LSSIM\n",
    "from modules.misc import ssim_metric, psnrb_metric\n",
    "from modules.ImageMetrics.metrics import three_ssim\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import mlflow.keras\n",
    "\n",
    "from models.autoEncoder import autoEncoder\n",
    "from models.ResidualAutoencoder import residualAutoEncoder\n",
    "from models.Unet import unet\n",
    "\n",
    "from multiprocessing import Process\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the datasets\n",
    "tinyDataSet, cifarDataSet, cifarAndTinyDataSet = DataSet(), DataSet(), DataSet()\n",
    "\n",
    "tinyDataSet = tinyDataSet.load_rafael_tinyImagenet_64x64_noise_data()\n",
    "cifarDataSet = cifarDataSet.load_rafael_cifar_10_noise_data()\n",
    "\n",
    "# concatenates the datasets\n",
    "cifarAndTinyDataSet = cifarAndTinyDataSet.concatenateDataSets(cifarDataSet, tinyDataSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: \n",
    "# paralelize the training (does it's necessary?)\n",
    "# batch size shoudn't be specified (keras API doc), does it affect the training?\n",
    "\n",
    "# training with LSSIM loss function and ssim and psnrb metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles all the models\n",
    "\n",
    "autoEncoder.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, psnrb_metric, three_ssim])\n",
    "unet.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, psnrb_metric, three_ssim])\n",
    "residualAutoEncoder.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, psnrb_metric, three_ssim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 15\n",
    "# how to decide the number of epochs and batch size?\n",
    "\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "# function to parallelize the training\n",
    "def train_model_paralel(model):\n",
    "        # trains the models with the datasets\n",
    "        print(model.name)\n",
    "        for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
    "                with mlflow.start_run(run_name= model.name +dataset.name):\n",
    "\n",
    "                        history = model.fit(\n",
    "                                x = dataset.x_train,\n",
    "                                y = dataset.y_train,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs,\n",
    "                                verbose = 1,\n",
    "                                validation_split = 0,\n",
    "                                shuffle = True,\n",
    "                                class_weight = None,\n",
    "                                sample_weight = None,\n",
    "                                steps_per_epoch = None,\n",
    "                                validation_steps = None,\n",
    "                                validation_batch_size = None,\n",
    "                                validation_freq = 1,\n",
    "                                max_queue_size = 10,\n",
    "                                workers = 1,\n",
    "                                use_multiprocessing = False\n",
    "                        )\n",
    "\n",
    "                        score = model.evaluate(dataset.x_test, dataset.y_test, verbose = 1)\n",
    "\n",
    "                        try:\n",
    "                                model.save_weights(\"models/weights/run1/\" + model.name + dataset.name + \".h5\")\n",
    "                        except:\n",
    "                                print(\"Error saving weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.now()\n",
    "\n",
    "procs = []\n",
    "\n",
    "# launches the training in parallel\n",
    "for model in [autoEncoder]:#, unet, residualAutoEncoder]:\n",
    "        proc = Process(target=train_model_paralel, args=(model, ))\n",
    "        proc.start()\n",
    "        procs.append(proc)\n",
    "\n",
    "# waits for the training to finish\n",
    "for proc in procs:\n",
    "        proc.join()\n",
    "\n",
    "end_date = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/run1.txt\", \"w\") as file:\n",
    "        file.write(\"Start date: \" + str(start_date) + \"\\n\")\n",
    "        file.write(\"End date: \" + str(end_date) + \"\\n\")\n",
    "        file.write(\"Duration: \" + str(end_date - start_date) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
