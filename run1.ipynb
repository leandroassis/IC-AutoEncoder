{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os import getcwd, environ\n",
    "\n",
    "path.insert(0, getcwd())\n",
    "path.insert(0, getcwd() + \"/modules/\")\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,1\"\n",
    "\n",
    "from modules.DataMod import DataSet\n",
    "from modules.CustomLosses import LSSIM, LPSNRB, L3SSIM\n",
    "from modules.misc import ssim_metric, psnrb_metric\n",
    "from modules.ImageMetrics.metrics import three_ssim\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras import models\n",
    "\n",
    "import mlflow.keras\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the datasets\n",
    "tinyDataSet, cifarDataSet, cifarAndTinyDataSet = DataSet(), DataSet(), DataSet()\n",
    "\n",
    "tinyDataSet = tinyDataSet.load_rafael_tinyImagenet_64x64_noise_data()\n",
    "cifarDataSet = cifarDataSet.load_rafael_cifar_10_noise_data()\n",
    "\n",
    "# concatenates the datasets\n",
    "cifarAndTinyDataSet = cifarAndTinyDataSet.concatenateDataSets(cifarDataSet, tinyDataSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: \n",
    "# paralelize the training (does it's necessary?)\n",
    "# batch size shoudn't be specified (keras API doc), does it affect the training?\n",
    "\n",
    "# training with LSSIM loss function and ssim and psnrb metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix bath_size and epochs (how to decide the number of epochs and batch size?)\n",
    "batch_size = 20\n",
    "epochs = 15\n",
    "\n",
    "file = open(\"logs/run1.txt\", \"w\")\n",
    "\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "# to do: paralelize the training\n",
    "def train_model_paralel(dataset : DataSet):\n",
    "        # trains the models with the datasets\n",
    "\n",
    "        for idx in range(3):\n",
    "                if idx == 0:\n",
    "                        model = models.load_model(\"nNet_models/AutoEncoder-2.3-64x64.json\", compile=False)\n",
    "                elif idx == 1:\n",
    "                        model = models.load_model(\"nNet_models/GANResidualAutoEncoder-0.1-64x64.json\", compile=False)\n",
    "                else:\n",
    "                        model = models.load_model(\"nNet_models/Unet2.3-64x64.json\", compile=False)\n",
    "\n",
    "                for loss in [L3SSIM(), LSSIM(), LPSNRB()]:\n",
    "\n",
    "                        try:\n",
    "                                model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, three_ssim, psnrb_metric])\n",
    "                        except Exception as e:\n",
    "                                file.write(f\"Error {e}: Error compiling {model.name} with {dataset.name} dataset\\n\")\n",
    "                                file.write(e.__cause__)\n",
    "                                file.write(e.__context__)\n",
    "                                continue\n",
    "                        \n",
    "\n",
    "                        with mlflow.start_run(run_name= model.name + dataset.name):\n",
    "                                \n",
    "                                try:\n",
    "                                        history = model.fit(\n",
    "                                                x = dataset.x_train,\n",
    "                                                y = dataset.y_train,\n",
    "                                                batch_size = batch_size,\n",
    "                                                epochs = epochs,\n",
    "                                                verbose = 1,\n",
    "                                                validation_split = 0,\n",
    "                                                shuffle = True,\n",
    "                                                class_weight = None,\n",
    "                                                sample_weight = None,\n",
    "                                                steps_per_epoch = None,\n",
    "                                                validation_steps = None,\n",
    "                                                validation_batch_size = None,\n",
    "                                                validation_freq = 1,\n",
    "                                                max_queue_size = 10,\n",
    "                                                workers = 1,\n",
    "                                                use_multiprocessing = False\n",
    "                                        )\n",
    "\n",
    "                                        model.save_weights(\"models/weights/run1/\" + model.name + dataset.name + \".h5\")\n",
    "\n",
    "                                except Exception as e:\n",
    "                                        file.write(f\"Error {e}: Error fitting and saving {model.name} with {dataset.name} dataset\\n\")\n",
    "                                        file.write(e.__cause__)\n",
    "                                        file.write(e.__context__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = []\n",
    "\n",
    "for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
    "        proc = multiprocessing.Process(target=train_model_paralel, args=(dataset, ))\n",
    "        procs.append(proc)\n",
    "\n",
    "# waits for the training to finish\n",
    "for proc in procs:\n",
    "        proc.start()\n",
    "        proc.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
