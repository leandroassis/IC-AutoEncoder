{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 21:43:02.316138: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-23 21:43:02.398652: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-23 21:43:02.399756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-23 21:43:04.388616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "637501\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "32748\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "from os import getcwd, environ\n",
    "\n",
    "path.insert(0, getcwd())\n",
    "path.insert(0, getcwd() + \"/modules\")\n",
    "from modules.DataMod import DataSet\n",
    "from modules.CustomLosses import LSSIM\n",
    "from modules.misc import ssim_metric, psnrb_metric\n",
    "from modules.ImageMetrics.metrics import three_ssim\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import mlflow.keras\n",
    "\n",
    "from models.autoEncoder import autoEncoder\n",
    "from models.ResidualAutoencoder import residualAutoEncoder\n",
    "from models.Unet import unet\n",
    "\n",
    "from multiprocessing import Process\n",
    "from datetime import datetime\n",
    "\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the datasets\n",
    "tinyDataSet, cifarDataSet, cifarAndTinyDataSet = DataSet(), DataSet(), DataSet()\n",
    "\n",
    "tinyDataSet = tinyDataSet.load_rafael_tinyImagenet_64x64_noise_data()\n",
    "cifarDataSet = cifarDataSet.load_rafael_cifar_10_noise_data()\n",
    "\n",
    "# concatenates the datasets\n",
    "cifarAndTinyDataSet = cifarAndTinyDataSet.concatenateDataSets(cifarDataSet, tinyDataSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: \n",
    "# paralelize the training (does it's necessary?)\n",
    "# batch size shoudn't be specified (keras API doc), does it affect the training?\n",
    "\n",
    "# training with LSSIM loss function and ssim and psnrb metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles all the models\n",
    "\n",
    "autoEncoder.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, psnrb_metric, three_ssim])\n",
    "unet.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, psnrb_metric, three_ssim])\n",
    "residualAutoEncoder.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = LSSIM(), metrics = [ssim_metric, psnrb_metric, three_ssim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 15\n",
    "# how to decide the number of epochs and batch size?\n",
    "\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "# function to parallelize the training\n",
    "def train_model_paralel(model):\n",
    "        # trains the models with the datasets\n",
    "        print(model.name)\n",
    "        for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
    "                with mlflow.start_run(run_name= model.name +dataset.name):\n",
    "\n",
    "                        history = model.fit(\n",
    "                                x = dataset.x_train,\n",
    "                                y = dataset.y_train,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs,\n",
    "                                verbose = 1,\n",
    "                                validation_split = 0,\n",
    "                                shuffle = True,\n",
    "                                class_weight = None,\n",
    "                                sample_weight = None,\n",
    "                                steps_per_epoch = None,\n",
    "                                validation_steps = None,\n",
    "                                validation_batch_size = None,\n",
    "                                validation_freq = 1,\n",
    "                                max_queue_size = 10,\n",
    "                                workers = 1,\n",
    "                                use_multiprocessing = False\n",
    "                        )\n",
    "\n",
    "                        score = model.evaluate(dataset.x_test, dataset.y_test, verbose = 1)\n",
    "\n",
    "                        try:\n",
    "                                model.save_weights(\"models/weights/run1/\" + model.name + dataset.name + \".h5\")\n",
    "                        except:\n",
    "                                print(\"Error saving weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder-2.3-64x64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet2.3-64x64"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-2:\n",
      "  File \"/tmp/ipykernel_7027/3828286023.py\", line 11, in train_model_paralel\n",
      "    for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualAutoEncoder-0.1-64x64"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "NameError: name 'tinyDataSet' is not defined\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-3:\n",
      "  File \"/tmp/ipykernel_7027/3828286023.py\", line 11, in train_model_paralel\n",
      "    for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
      "NameError: name 'tinyDataSet' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_7027/3828286023.py\", line 11, in train_model_paralel\n",
      "    for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
      "NameError: name 'tinyDataSet' is not defined\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.now()\n",
    "\n",
    "procs = []\n",
    "\n",
    "# launches the training in parallel\n",
    "for model in [autoEncoder, unet, residualAutoEncoder]:#, unet, residualAutoEncoder]:\n",
    "        proc = Process(target=train_model_paralel, args=(model, ))\n",
    "        proc.start()\n",
    "        procs.append(proc)\n",
    "\n",
    "# waits for the training to finish\n",
    "for proc in procs:\n",
    "        proc.join()\n",
    "\n",
    "end_date = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/run1.txt\", \"w\") as file:\n",
    "        file.write(\"Start date: \" + str(start_date) + \"\\n\")\n",
    "        file.write(\"End date: \" + str(end_date) + \"\\n\")\n",
    "        file.write(\"Duration: \" + str(end_date - start_date) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
