{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os import getcwd, environ\n",
    "\n",
    "path.insert(0, getcwd())\n",
    "path.insert(0, getcwd() + \"/modules/\")\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,1\"\n",
    "\n",
    "from modules.DataMod import DataSet\n",
    "from modules.CustomLosses import LSSIM, LPSNRB, L3SSIM\n",
    "from modules.misc import ssim_metric\n",
    "from modules.ImageMetrics.metrics import three_ssim, psnrb\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras import models\n",
    "\n",
    "import mlflow.keras\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the datasets\n",
    "tinyDataSet, cifarDataSet, cifarAndTinyDataSet = DataSet(), DataSet(), DataSet()\n",
    "\n",
    "tinyDataSet = tinyDataSet.load_rafael_tinyImagenet_64x64_noise_data()\n",
    "cifarDataSet = cifarDataSet.load_rafael_cifar_10_noise_data()\n",
    "\n",
    "# concatenates the datasets\n",
    "cifarAndTinyDataSet = cifarAndTinyDataSet.concatenateDataSets(cifarDataSet, tinyDataSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix bath_size and epochs (how to decide the number of epochs and batch size?)\n",
    "batch_size = 20\n",
    "epochs = 15\n",
    "\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "# trains a model with a datasets\n",
    "def train_model(model, dataset : DataSet):\n",
    "\n",
    "        file = open(\"logs/run1.txt\", \"w\")\n",
    "\n",
    "        # training for each loss\n",
    "        losses = {\"LSSIM\":LSSIM(), \"LPSNRB\":LPSNRB(), \"L3SSIM\":L3SSIM()}\n",
    "\n",
    "        for loss in losses:\n",
    "\n",
    "                try:\n",
    "                        model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False), loss = losses[loss], metrics = [ssim_metric, three_ssim, psnrb])\n",
    "                except Exception as e:\n",
    "                        file.write(f\"Error {e}: Error compiling {model.name} with {dataset.name} dataset\\n\")\n",
    "                        continue\n",
    "                \n",
    "\n",
    "                with mlflow.start_run(run_name= model.name + dataset.name):\n",
    "                        \n",
    "                        try:\n",
    "                                history = model.fit(\n",
    "                                        x = dataset.x_train,\n",
    "                                        y = dataset.y_train,\n",
    "                                        batch_size = batch_size,\n",
    "                                        epochs = epochs,\n",
    "                                        verbose = 1,\n",
    "                                        validation_split = 0,\n",
    "                                        shuffle = True,\n",
    "                                        class_weight = None,\n",
    "                                        sample_weight = None,\n",
    "                                        steps_per_epoch = None,\n",
    "                                        validation_steps = None,\n",
    "                                        validation_batch_size = None,\n",
    "                                        validation_freq = 1,\n",
    "                                        max_queue_size = 10,\n",
    "                                        workers = 1,\n",
    "                                        use_multiprocessing = False\n",
    "                                )\n",
    "\n",
    "                                model.save_weights(\"models/weights/run1/\" + model.name + dataset.name + loss +\".h5\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                                file.write(f\"Error {e}: Error fitting and saving {model.name} with {dataset.name} dataset\\n\")\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = []\n",
    "\n",
    "\n",
    "# to do: paralelize the training\n",
    "'''\n",
    "multiprocessing.set_start_method('spawn')\n",
    "\n",
    "for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
    "        proc = multiprocessing.Process(target=train_model, args=(dataset, ))\n",
    "        procs.append(proc)\n",
    "\n",
    "# waits for the training to finish\n",
    "for proc in procs:\n",
    "        proc.start()\n",
    "        proc.join()\n",
    "'''\n",
    "\n",
    "for path in [\"models/arch/AutoEncoder-2.3-64x64.json\", \"models/arch/GANResidualAutoEncoder-0.1-64x64.json\", \"models/arch/Unet2.3-64x64.json\"]:\n",
    "        # reads the model\n",
    "        with open(path, \"r\") as json_file:\n",
    "                model = models.model_from_json(json_file.read())\n",
    "        \n",
    "        for dataset in [tinyDataSet, cifarDataSet, cifarAndTinyDataSet]:\n",
    "                train_model(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
